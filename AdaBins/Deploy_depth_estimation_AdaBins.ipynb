{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1c706ee",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5e19d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import boto3\n",
    "import os\n",
    "import sagemaker\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f1b0009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.predictor import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f160075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import strftime,gmtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cad2ccd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "429a8f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "sns_client = boto3.client('sns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8da8b6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = Session()\n",
    "region = sess.boto_region_name\n",
    "bucket = sess.default_bucket()\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "377c6023",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = boto3.session.Session()\n",
    "sm_runtime = boto_session.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4257348e",
   "metadata": {},
   "source": [
    "## Load model drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34065551",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dff1c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1HMgff-FV6qw1L0ywQZJ7ECa9VPq1bIoj\n",
      "To: /home/ec2-user/SageMaker/Depth_Estimation/AdaBins/AdaBins_kitti.pt\n",
      "100%|████████████████████████████████████████| 941M/941M [00:11<00:00, 84.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown https://drive.google.com/uc?id=1HMgff-FV6qw1L0ywQZJ7ECa9VPq1bIoj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "021955a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv AdaBins_kitti.pt model.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0301c9",
   "metadata": {},
   "source": [
    "### Create model archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54fb930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_archive_name = 'Adabins-async.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54f71d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.pth\n",
      "code/\n",
      "code/models/\n",
      "code/models/unet_adaptive_bins.py\n",
      "code/models/layers.py\n",
      "code/models/miniViT.py\n",
      "code/models/__init__.py\n",
      "code/model_io.py\n",
      "code/requirements.txt\n",
      "code/inference.py\n",
      "code/.ipynb_checkpoints/\n"
     ]
    }
   ],
   "source": [
    "!tar -cvzf {model_archive_name} model.pth code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "005e23bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model uploaded to: s3://sagemaker-ca-central-1-333752261573/AdaBins_model/Adabins-async.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# model package tarball (model artifact + inference code)\n",
    "model_url = sess.upload_data(path=model_archive_name, key_prefix='AdaBins_model')\n",
    "print('model uploaded to: {}'.format(model_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cacbb62",
   "metadata": {},
   "source": [
    "### Create model and test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8794359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_version = '1.7.1'\n",
    "py_version = 'py36'\n",
    "env= {\n",
    "            'TS_MAX_REQUEST_SIZE': '100000000', #default max request size is 6 Mb for torchserve, need to update it to support the 70 mb input payload\n",
    "            'TS_MAX_RESPONSE_SIZE': '100000000',\n",
    "            'TS_DEFAULT_RESPONSE_TIMEOUT': '1000'\n",
    "        }\n",
    "\n",
    "sm_model = PyTorchModel(model_data=model_url,\n",
    "                               framework_version=framework_version,\n",
    "                               role=role,\n",
    "                               sagemaker_session=sess,\n",
    "                               entry_point='inference.py',\n",
    "                               source_dir= 'code',\n",
    "                               env=env,\n",
    "                               py_version=py_version\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaf2177d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "instance_type = 'ml.g4dn.xlarge'\n",
    "uncompiled_predictor = sm_model.deploy(initial_instance_count=1, instance_type=instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68a6846",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "725407f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import requests\n",
    "client = boto3.client('sagemaker-runtime', region_name=region)\n",
    "content_type = 'application/x-image'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1535edea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sample 2\n",
    "path= 'test_data/frame54.jpg'\n",
    "img = cv2.imread(path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "body = io.open(path, 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2a46594a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602.1256446838379\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "rv = client.invoke_endpoint(EndpointName=uncompiled_predictor.endpoint_name, Body=body, ContentType=content_type)\n",
    "t1 = time.time()\n",
    "\n",
    "time_elapsed = (t1-t0)*1000\n",
    "print(time_elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3f2dde56",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions= json.loads(rv['Body'].read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "899d10eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c7c7ad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_arr = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "35b21309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 480, 640)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00564d62",
   "metadata": {},
   "source": [
    "# Measure invocation time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6b998d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "627.171516418457\n",
      "653.822660446167\n",
      "683.9485168457031\n",
      "674.1800308227539\n",
      "655.482292175293\n",
      "665.8425331115723\n",
      "646.6827392578125\n",
      "651.9558429718018\n",
      "659.8081588745117\n",
      "652.277946472168\n"
     ]
    }
   ],
   "source": [
    "inferenceTime_list=[]\n",
    "for i in range(10):\n",
    "    body = io.open(path, 'rb')\n",
    "    t0 = time.time()\n",
    "    rv = client.invoke_endpoint(EndpointName=uncompiled_predictor.endpoint_name, Body=body, ContentType=content_type)\n",
    "    t1 = time.time()\n",
    "\n",
    "    time_elapsed = (t1-t0)*1000\n",
    "    inferenceTime_list.append(time_elapsed)\n",
    "    print(time_elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9119b195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "657.117223739624"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(inferenceTime_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4897332",
   "metadata": {},
   "source": [
    "# Async endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3978cf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.async_inference.async_inference_config import AsyncInferenceConfig\n",
    "\n",
    "bucket= 'gulogulo-inference-results'\n",
    "\n",
    "async_config = AsyncInferenceConfig(\n",
    "                output_path= f\"s3://{bucket}/output\",\n",
    "                max_concurrent_invocations_per_instance=2,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5425b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_version = '1.7.1'\n",
    "py_version = 'py36'\n",
    "env= {\n",
    "            'TS_MAX_REQUEST_SIZE' : '1000000000', #default max request size is 6 Mb for torchserve, need to update it to support the 70 mb input payload\n",
    "            'TS_MAX_RESPONSE_SIZE': '1000000000',\n",
    "            'TS_DEFAULT_RESPONSE_TIMEOUT': '1000'\n",
    "        }\n",
    "\n",
    "sm_model = PyTorchModel(model_data=model_url,\n",
    "                               framework_version=framework_version,\n",
    "                               role=role,\n",
    "                               sagemaker_session=sess,\n",
    "                               entry_point='inference.py',\n",
    "                               source_dir= 'code-async',\n",
    "                               env=env,\n",
    "                               py_version=py_version\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "777197e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "instance_type = 'ml.g4dn.xlarge'\n",
    "\n",
    "async_uncompiled_predictor = sm_model.deploy(async_inference_config=async_config,\n",
    "                                       initial_instance_count=1,\n",
    "                                       instance_type=instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b9dc55",
   "metadata": {},
   "source": [
    "# Test endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a2d56db",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name= ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07d80dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1_s3_location= ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a68517",
   "metadata": {},
   "source": [
    "### Invoke async endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b15c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm_runtime.invoke_endpoint_async(\n",
    "    EndpointName=endpoint_name, \n",
    "    InputLocation=input_1_s3_location)\n",
    "output_location = response['OutputLocation']\n",
    "print(f\"OutputLocation: {output_location}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
